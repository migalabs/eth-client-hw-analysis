{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# This notebook is thought to be executed one at a time.\n",
    "# Please read and execute from top to bottom in order.\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "#                                                                   #\n",
    "#         Configure here your parameters for the analysis           #\n",
    "#                                                                   #\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "\n",
    "# Goerli\n",
    "genesis_unix_time = 1616508000 \t\t# Constant\n",
    "start_epoch = 175200 \t\t\t\t# Start epoch you want to plot\n",
    "start_time = \"2023-05-11T06:00:00Z\" # Start time from where you want to download into CSV\n",
    "\n",
    "end_epoch = 176200 \t\t\t\t\t# End epoch you want to plot\n",
    "end_time = \"2023-05-15T18:40:00Z\"\t# End time from where you want to download into CSV\n",
    "duration = \"107h\"\t\t\t\t\t# Duration from start to end (download)\n",
    "network = \"goerli\"\t\t\t\t\t# Tag\n",
    "\n",
    "# Mainnet\n",
    "genesis_unix_time = 1606824023 \t\t# Constant\n",
    "start_epoch = 201700  \t\t\t\t# Start epoch you want to plot\n",
    "start_time = \"2023-05-16T22:40:00Z\"\t# Start time from where you want to download into CSV\n",
    "\n",
    "end_epoch = 202700\t\t\t\t\t# End epoch you want to plot\n",
    "end_time = \"2023-05-21T09:20:23Z\"\t# End time from where you want to download into CSV\n",
    "duration = \"107h\"\t\t\t\t\t# Duration from start to end (download)\n",
    "network=\"mainnet\"\t\t\t\t\t# Tag\n",
    "\n",
    "# We had some issues with the Teku instance so we had to relaunch it\n",
    "teku_start_time = \"2023-05-26T09:20:00Z\"\n",
    "teku_end_time = \"2023-05-28T07:20:00Z\"\n",
    "\n",
    "# Common\n",
    "prometheus_ip = \"localhost\"\t\t\t\t# From where to download Prometheus data\n",
    "project_name = \"resource-analysis-v2\"\t# Tag\n",
    "step = \"1s\"\t\t\t\t\t\t\t\t# Step to download data (you might need to increase if too much data: Prometheus allows 11K points)\n",
    "csv_folder = f\"\"\"./{network}/csv/\"\"\"\t# Where to store CSV files (download data)\n",
    "fig_folder = f\"\"\"./{network}/fig/\"\"\"\t# Where to store figures (plots)\n",
    "epoch_seconds = 384\t\t\t\t\t\t# Constant\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "#                                                                   #\n",
    "#                  End of cofiguration parameters\t\t\t\t    #\n",
    "#                                                                   #\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "\n",
    "def ask(query_str):\n",
    "\tresponse = requests.get('http://' + prometheus_ip + ':9091/api/v1/query', params={\n",
    "\t\t'query': query_str,\n",
    "\t\t'time': end_time}) \n",
    "\tresponse_json = response.json()\n",
    "\tresponse_data = response_json['data']['result']\n",
    "\treturn response_data\n",
    "\n",
    "def ask_range(query_str, i_start_time, i_end_time):\n",
    "\tresponse = requests.get('http://' + prometheus_ip + ':9091/api/v1/query_range', params={\n",
    "\t\t'query': query_str,\n",
    "\t\t'start': i_start_time,\n",
    "\t\t'end': i_end_time,\n",
    "\t\t'step': step}) \n",
    "\tresponse_json = response.json()\n",
    "\tresponse_data = response_json['data']['result']\n",
    "\treturn response_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "#                                                                   #\n",
    "#                          Query Definition\t\t\t\t            #\n",
    "#                                                                   #\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "\n",
    "# In our case we had to assign a client tag for each client depending on which IP the node was hosted\n",
    "\n",
    "\n",
    "ip_clients_map = {\n",
    "\t'0.0.0.0': 'Prysm',\n",
    "\t'0.0.0.0': 'Lighthouse',\n",
    "\t'0.0.0.0': 'Teku',\n",
    "\t'0.0.0.0': 'Nimbus',\n",
    "\t'0.0.0.0': 'Lodestar',\n",
    "}\n",
    "general_filter = {\n",
    "    'project_name': 'resource-analysis-v2',\n",
    "    'network': network,\n",
    "    'server': \"0.0.0.0|0.0.0.0|0.0.0.0|0.0.0.0|0.0.0.0\"\n",
    "}\n",
    "\n",
    "metrics = [\n",
    "\t{\n",
    "\t'group': 'cpu',\n",
    "\t'name': 'cpu_seconds_user',\n",
    "\t'ylabel': 'Percentage',\n",
    "    'ylim': [0, 100],\n",
    "    'xlabel': 'Epoch',\n",
    "    'title': 'CPU consumption',\n",
    "    'queries': [\n",
    "\t\t\t{\n",
    "\t\t\t\t'query': f\"\"\"rate(node_cpu_seconds_total[{step}]) * 100\"\"\",\n",
    "\t\t\t\t'filters': {\n",
    "\t\t\t\t\t'mode': 'user'\n",
    "\t\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t},\n",
    "\t{\n",
    "\t'group': 'memory',\n",
    "\t'name': 'used_mem',\n",
    "    'ylabel': 'Percentage',\n",
    "    'ylim': [0, 100],\n",
    "    'xlabel': 'Epoch',\n",
    "    'title': 'Used Memory',\n",
    "    'queries': [\n",
    "\t\t\t{\n",
    "\t\t\t\t'query': f\"\"\"(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100\"\"\",\n",
    "\t\t\t\t'filters': {}\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t},\n",
    "    {\n",
    "\t'group': 'network',\n",
    "\t'name': 'network_receive',\n",
    "    'ylabel': f\"\"\"MB/{step}\"\"\",\n",
    "    'ylim': [0, 10],\n",
    "    'xlabel': 'Epoch',\n",
    "    'title': 'Network Receive Rate',\n",
    "    'queries': [\n",
    "\t\t\t{\n",
    "\t\t\t\t'query': f\"\"\"rate(node_network_receive_bytes_total[{step}]) / 1024 / 1024\"\"\",\n",
    "\t\t\t\t'filters': {\n",
    "\t\t\t\t\t'device': 'eno1'\n",
    "\t\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t},\n",
    "    {\n",
    "\t'group': 'network',\n",
    "\t'name': 'network_transmit',\n",
    "    'ylabel': f\"\"\"MB/{step}\"\"\",\n",
    "    'ylim': [0, 2],\n",
    "    'xlabel': 'Epoch',\n",
    "    'title': 'Network Send Rate',\n",
    "    'queries': [\n",
    "\t\t\t{\n",
    "\t\t\t\t'query': f\"\"\"rate(node_network_transmit_bytes_total[{step}]) / 1024 / 1024\"\"\",\n",
    "\t\t\t\t'filters': {\n",
    "\t\t\t\t\t'device': 'eno1'\n",
    "\t\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t},\n",
    "    {\n",
    "\t'group': 'peers',\n",
    "\t'name': 'libp2p_peers',\n",
    "    'ylabel': f\"\"\"Count\"\"\",\n",
    "    'ylim': [0, 200],\n",
    "    'xlabel': 'Epoch',\n",
    "    'title': 'Lib P2P Peers',\n",
    "    'queries': [\n",
    "\t\t\t{\n",
    "\t\t\t\t'query': f\"\"\"libp2p_peers\"\"\",\n",
    "\t\t\t\t'filters': {}\n",
    "\t\t\t},\n",
    "            {\n",
    "\t\t\t\t'query': f\"\"\"p2p_peer_count\"\"\",\n",
    "\t\t\t\t'filters': {\n",
    "                    'state': 'Connected'\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t},\n",
    "    {\n",
    "\t'group': 'disk',\n",
    "\t'name': 'disk_write_bytes',\n",
    "    'ylabel': f\"\"\"MB/m\"\"\",\n",
    "    'ylim': [0, 10],\n",
    "    'xlabel': 'Epoch',\n",
    "    'title': 'Disk Write Bytes Ratio',\n",
    "    'queries': [\n",
    "\t\t\t{\n",
    "\t\t\t\t'query': f\"\"\"rate(node_disk_written_bytes_total[{step}]) / 1024 / 1024\"\"\",\n",
    "\t\t\t\t'filters': {\n",
    "                    'device': \"nvme0n1\"\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t},\n",
    "    {\n",
    "\t'group': 'disk',\n",
    "\t'name': 'disk_read',\n",
    "    'ylabel': f\"\"\"MB/m\"\"\",\n",
    "    'ylim': [0, 10],\n",
    "    'xlabel': 'Epoch',\n",
    "    'title': 'Disk Read Bytes Ratio',\n",
    "    'queries': [\n",
    "\t\t\t{\n",
    "\t\t\t\t'query': f\"\"\"rate(node_disk_read_bytes_total[{step}]) / 1024 / 1024\"\"\",\n",
    "\t\t\t\t'filters': {\n",
    "                    'device': \"nvme0n1\"\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t},\n",
    "    {\n",
    "\t'group': 'disk',\n",
    "\t'name': 'disk_read_ops',\n",
    "    'ylabel': f\"\"\"OPS/m\"\"\",\n",
    "    'ylim': [0, 10],\n",
    "    'xlabel': 'Epoch',\n",
    "    'title': 'Disk Read Ops Ratio',\n",
    "    'queries': [\n",
    "\t\t\t{\n",
    "\t\t\t\t'query': f\"\"\"rate(node_disk_reads_completed_total[{step}]) / 1024 / 1024\"\"\",\n",
    "\t\t\t\t'filters': {\n",
    "                    'device': \"nvme0n1\"\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t},\n",
    "    {\n",
    "\t'group': 'disk',\n",
    "\t'name': 'disk_write_ops',\n",
    "    'ylabel': f\"\"\"OPS/m\"\"\",\n",
    "    'ylim': [0, 10],\n",
    "    'xlabel': 'Epoch',\n",
    "    'title': 'Disk Write Ops Ratio',\n",
    "    'queries': [\n",
    "\t\t\t{\n",
    "\t\t\t\t'query': f\"\"\"rate(node_disk_writes_completed_total[{step}]) / 1024 / 1024\"\"\",\n",
    "\t\t\t\t'filters': {\n",
    "                    'device': \"nvme0n1\"\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t},\n",
    "]\n",
    "\n",
    "\n",
    "# Metrics\n",
    "\n",
    "# METRIC_MEMORY = 'node_memory_MemAvailable_bytes'\n",
    "# METRIC_MEMORYTOTAL = 'node_memory_MemTotal_bytes'\n",
    "# METRIC_CPU = 'node_cpu_seconds_total'\n",
    "# METRIC_NETRECV = 'node_network_receive_bytes_total'\n",
    "# METRIC_NETSENT = 'node_network_transmit_bytes_total'\n",
    "# METRIC_NETRECV_PACK = 'node_network_receive_packets_total'\n",
    "# METRIC_NETSENT_PACK = 'node_network_transmit_packets_total'\n",
    "# METRIC_SLOT = 'beacon_head_slot'\n",
    "# METRIC_PEERS = 'libp2p_peers'\n",
    "# METRIC_DISKTOTAL = \"node_filesystem_size_bytes\"\n",
    "# METRIC_DISKAVAIL = \"node_filesystem_avail_bytes\"\n",
    "# METRIC_DISKWRITE_BYTES = \"node_disk_written_bytes_total\"\n",
    "# METRIC_DISKREAD_BYTES = \"node_disk_read_bytes_total\"\n",
    "# METRIC_DISKWRITE_OPS = \"node_disk_writes_completed_total\"\n",
    "# METRIC_DISKREAD_OPS = \"node_disk_reads_completed_total\"\n",
    "# METRIC_EARNS = \"validators_epoch_earned_amount_metrics\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "#                                                                   #\n",
    "#                          Data Collection\t\t\t\t            #\n",
    "#                                                                   #\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "\n",
    "df_result = []\n",
    "\n",
    "for metric in metrics:\n",
    "\tprint(\"Requesting metric: \", metric)\n",
    "\ttmp_csv = []\n",
    "\tfor query in metric['queries']:\n",
    "\t\t\n",
    "\t\tresponse_data = ask_range(query['query'], start_time, end_time)\n",
    "\t\t\n",
    "\n",
    "\t\tfor item in response_data:\n",
    "\t\t\tpass_filter = True\n",
    "\t\t\t# dismiss wrong metrics\n",
    "\t\t\tfilters = general_filter | query['filters']\n",
    "\t\t\tfor key, value in filters.items():\n",
    "\t\t\t\tif key in item['metric']:\n",
    "\t\t\t\t\trequested = item['metric'][key]\n",
    "\t\t\t\t\tif not re.match(value, requested):\n",
    "\t\t\t\t\t\tpass_filter = False # if any filter does not match, do not include this series\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tpass_filter = False\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\tif pass_filter:\n",
    "\t\t\t\tfor value in item['values']:\n",
    "\t\t\t\t\tip = item['metric']['server']\n",
    "\t\t\t\t\ttuple = [str(ip_clients_map[ip]), str(value[0]), float(value[1])]\n",
    "\t\t\t\t\ttmp_csv.append(tuple)\n",
    "\n",
    "\t\t# Teku exceptional case\n",
    "\t\tif network == 'mainnet':\n",
    "\t\t\tresponse_data_teku = ask_range(query['query'], teku_start_time, teku_end_time)\n",
    "\t\t\tfor item in response_data_teku:\n",
    "\t\t\t\tpass_filter = True\n",
    "\t\t\t\t# dismiss wrong metrics\n",
    "\t\t\t\tfilters = general_filter | query['filters']\n",
    "\t\t\t\tif item['metric']['server'] != \"0.0.0.0\":\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tfor key, value in filters.items():\n",
    "\t\t\t\t\tif key in item['metric']:\n",
    "\t\t\t\t\t\trequested = item['metric'][key]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tpass_filter = False\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif not re.match(value, requested):\n",
    "\t\t\t\t\t\tpass_filter = False # if any filter does not match, do not include this serie\n",
    "\t\t\t\tif pass_filter:\n",
    "\t\t\t\t\tfor value in item['values']:\n",
    "\t\t\t\t\t\tip = item['metric']['server']\n",
    "\t\t\t\t\t\ttuple = [str(ip_clients_map[ip]), str(value[0]), float(value[1])]\n",
    "\t\t\t\t\t\ttmp_csv.append(tuple)\n",
    "\t\n",
    "\tdf = pd.DataFrame(tmp_csv, columns=['client', 'timestamp', 'metric'])\n",
    "\tdf = df.groupby(by=['client', 'timestamp']).agg({\n",
    "\t\t'metric': 'mean'\n",
    "\t}).rename(columns={\"metric\": metric['name']})\n",
    "\t\n",
    "\n",
    "\tif len(df_result) == 0:\n",
    "\t\tdf_result = df\n",
    "\telse:\n",
    "\t\tdf_result = df_result.merge(df, how='outer', on=['client', 'timestamp'])\n",
    "df_result = df_result.reset_index()\n",
    "df_result['timestamp'] = df_result['timestamp'].astype(float)\n",
    "df_result['epoch'] = ((df_result['timestamp'] - float(genesis_unix_time)) / float(epoch_seconds)).astype(int)\n",
    "for client in ip_clients_map.values():\n",
    "\tdf_client=df_result[df_result.client == client]\n",
    "\tdf_client.to_csv(f\"\"\"{csv_folder}/{client}.csv\"\"\")\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "#                                                                   #\n",
    "#                          Data Plotting\t\t\t\t            #\n",
    "#                                                                   #\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #\n",
    "\n",
    "colors = {\n",
    "    'Prysm': 'red',\n",
    "    'Lighthouse': 'blue',\n",
    "    'Teku': 'orange',\n",
    "    'Nimbus': 'green',\n",
    "    'Lodestar': 'purple'\n",
    "}\n",
    "\n",
    "df_prysm = pd.read_csv(f\"{csv_folder}/Prysm.csv\")\n",
    "df_lighthouse = pd.read_csv(f\"{csv_folder}/Lighthouse.csv\")\n",
    "df_teku = pd.read_csv(f\"{csv_folder}/Teku.csv\")\n",
    "df_nimbus = pd.read_csv(f\"{csv_folder}/Nimbus.csv\")\n",
    "df_lodestar = pd.read_csv(f\"{csv_folder}/Lodestar.csv\")\n",
    "\n",
    "if network == 'mainnet': \n",
    "\tdf_teku['timestamp'] = df_teku['timestamp'].apply(lambda x: (x - 432000) if x > 1684660800 else x)\n",
    "\tdf_teku.drop(df_teku[(df_teku['timestamp'] > 1684438620) & (df_teku['timestamp'] < 1684604340)].index, inplace = True)\n",
    "\tdf_teku['timestamp'] = df_teku['timestamp'].apply(lambda x: (x - 165720) if x > 1684438620 else x)\n",
    "\n",
    "\tdf_teku['epoch'] = ((df_teku['timestamp'] - float(genesis_unix_time)) / float(epoch_seconds)).astype(int)\n",
    "\n",
    "dfs = [df_prysm, df_lighthouse, df_teku, df_nimbus, df_lodestar]\n",
    "df_all = pd.concat(dfs)\n",
    "dfs.append(df_all)\n",
    "markers_dict = {\"Prysm\": \".\", \"Lighthouse\": \"1\", \"Teku\": \"2\", \"Nimbus\": \"3\", \"Lodestar\": \"4\"}\n",
    "\n",
    "for metric in metrics:\n",
    "    print(\"Analyzing metric: \", metric)\n",
    "    for df, client_name in zip(dfs, ['prysm', 'lighthouse', 'teku', 'nimbus', 'lodestar', 'all']):\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "        g = sns.scatterplot(data=df, x=\"epoch\", y=metric['name'], \n",
    "                            hue='client', \n",
    "                            palette=colors,\n",
    "                            style='client')\n",
    "        g.set(xlabel=metric['xlabel'], ylabel=metric['ylabel'], title=f\"\"\"{metric['title']} ({network})\"\"\", ylim=metric['ylim'])\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"\"\"{fig_folder}/{metric['name']}_{client_name}\"\"\")\n",
    "        plt.close(fig)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
